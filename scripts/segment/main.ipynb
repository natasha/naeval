{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run -n main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -n main.py\n",
    "# for name in [SENT, TOKEN]:\n",
    "#     path = join_path(DATA_DIR, name, DATASET)\n",
    "#     !mkdir -p {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -n main.py\n",
    "# for type in [TOKEN, SENT]:\n",
    "#     log(type)\n",
    "#     for name in [CORPORA, SYNTAG, GICRYA, RNC]:\n",
    "#         paths = (\n",
    "#             join_path(CORUS_DATA_DIR, _)\n",
    "#             for _ in CORUS_FILES[name]\n",
    "#         )\n",
    "#         records = (\n",
    "#             record\n",
    "#             for path in paths\n",
    "#             for record in DATASETS[name](path)\n",
    "#         )\n",
    "#         records = log_progress(records, desc=name)\n",
    "#         partitions = PARSES[type][name](records)\n",
    "#         lines = format_partitions(partitions)\n",
    "#         path = join_path(DATA_DIR, type, DATASET, name + JL + GZ)\n",
    "#         dump_gz_lines(lines, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for type in [TOKEN, SENT]:\n",
    "#     for name in MODELS[type]:\n",
    "#         path = join_path(DATA_DIR, type, name)\n",
    "#         !mkdir -p {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rusenttokenize\n",
    "# !pip install segtok\n",
    "# !pip install mosestokenizer\n",
    "# !pip install razdel\n",
    "\n",
    "# !pip install spacy\n",
    "# !pip install https://github.com/aatimofeev/spacy_russian_tokenizer/archive/master.zip#egg=python-simhash\n",
    "# !pip install https://github.com/Koziev/rutokenizer/archive/master.zip#egg=rutokenizer\n",
    "# !pip install pymystem3\n",
    "\n",
    "# !pip install nltk\n",
    "# nltk.download('punkt')\n",
    "# !wget https://raw.githubusercontent.com/mhq/train_punkt/master/russian.pickle -O ~/nltk_data/tokenizers/punkt/PY3/russian.pickle\n",
    "\n",
    "# Texterra\n",
    "# Можно ещё сравнивать с https://texterra.ispras.ru/products, но\n",
    "# 1. она медленно работает, как минимум затраты на http\n",
    "# 2. иногда кидает ошибку (возможно дело в английских предложениях)\n",
    "# 3. качество немного выше segtok\n",
    "\n",
    "# Polyglot\n",
    "# реализует http://www.unicode.org/reports/tr29/\n",
    "\n",
    "# Сорян, не смог установить. Дикие траблы с ICU\n",
    "# brew install icu4c\n",
    "# export ICU_VERSION=62.1\n",
    "# export BASE=/usr/local/Cellar/icu4c/\n",
    "# export PATH=$PATH:$BASE/$ICU_VERSION/bin\n",
    "# export PYICU_INCLUDES=$BASE/$ICU_VERSION/include\n",
    "# export PYICU_LFLAGS=-L$BASE/$ICU_VERSION/lib\n",
    "# pip install pyicu polyglot\n",
    "\n",
    "# Вроде установилось но \n",
    "# > from polyglot.text import Text\n",
    "# > Text('...')\n",
    "# Symbol not found: __ZNK6icu_6214Transliterator12getTargetSetERNS_10UnicodeSetE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for type in [SENT, TOKEN]:\n",
    "#     log(type)\n",
    "#     for model_name in MODELS[type]:\n",
    "#         log(model_name)\n",
    "#         model = MODELS[type][model_name]\n",
    "#         if is_class(model):\n",
    "#             model = model()\n",
    "#         model = Timing(model)\n",
    "\n",
    "#         for dataset_name in DATASETS:\n",
    "#             path = join_path(DATA_DIR, type, DATASET, dataset_name + JL + GZ)\n",
    "#             lines = load_gz_lines(path)\n",
    "#             records = parse_partitions(lines)\n",
    "#             records = head(records, 10000)\n",
    "#             records = log_progress(records, desc=dataset_name)\n",
    "\n",
    "#             records = (\n",
    "#                 Partition.from_substrings(model(_.text))\n",
    "#                 for _ in records\n",
    "#             )\n",
    "#             path = join_path(DATA_DIR, type, model_name, dataset_name + JL + GZ)\n",
    "#             lines = format_partitions(records)\n",
    "#             dump_gz_lines(lines, path)\n",
    "\n",
    "#             path = join_path(DATA_DIR, STATS + JL)\n",
    "#             record = [[type, model_name, dataset_name], model.time]\n",
    "#             lines = format_jl([record])\n",
    "#             append_lines(lines, path)\n",
    "\n",
    "#             model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "keys = [\n",
    "    (type, model, dataset)\n",
    "    for type in [TOKEN, SENT]\n",
    "    for model in MODELS[type]\n",
    "    for dataset in DATASETS\n",
    "]\n",
    "for type, model, dataset in log_progress(keys):\n",
    "    path = join_path(DATA_DIR, type, DATASET, dataset + JL + GZ)\n",
    "    lines = load_gz_lines(path)\n",
    "    targets = parse_partitions(lines)\n",
    "\n",
    "    path = join_path(DATA_DIR, type, model, dataset + JL + GZ)\n",
    "    lines = load_gz_lines(path)\n",
    "    preds = parse_partitions(lines)\n",
    "\n",
    "    score = score_partitions(preds, targets)\n",
    "    scores[type, model, dataset] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = join_path(DATA_DIR, STATS + JL)\n",
    "lines = load_lines(path)\n",
    "items = parse_jl(lines)\n",
    "times = {\n",
    "    tuple(key): time\n",
    "    for key, time in items\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = report_table(scores, times, DATASETS, MODELS, TOKEN)\n",
    "html = format_report(table, github=True)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = report_table(scores, times, DATASETS, MODELS, SENT)\n",
    "html = format_report(table, github=True)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
