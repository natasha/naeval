{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run -n main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -n main.py\n",
    "# for name in [SENT, TOKEN]:\n",
    "#     path = join_path(DATA_DIR, name, DATASET)\n",
    "#     !mkdir -p {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -n main.py\n",
    "# for type in [TOKEN, SENT]:\n",
    "#     log(type)\n",
    "#     for name in [CORPORA, SYNTAG, GICRYA, RNC]:\n",
    "#         paths = (\n",
    "#             join_path(CORUS_DATA_DIR, _)\n",
    "#             for _ in CORUS_FILES[name]\n",
    "#         )\n",
    "#         records = (\n",
    "#             record\n",
    "#             for path in paths\n",
    "#             for record in DATASETS[name](path)\n",
    "#         )\n",
    "#         records = log_progress(records, desc=name)\n",
    "#         partitions = PARSES[type][name](records)\n",
    "#         lines = format_partitions(partitions)\n",
    "#         path = join_path(DATA_DIR, type, DATASET, name + JL + GZ)\n",
    "#         dump_gz_lines(lines, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for type in [TOKEN, SENT]:\n",
    "#     for name in MODELS[type]:\n",
    "#         path = join_path(DATA_DIR, type, name)\n",
    "#         !mkdir -p {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rusenttokenize\n",
    "# !pip install segtok\n",
    "# !pip install mosestokenizer\n",
    "# !pip install razdel\n",
    "\n",
    "# !pip install spacy\n",
    "# !pip install https://github.com/aatimofeev/spacy_russian_tokenizer/archive/master.zip#egg=python-simhash\n",
    "# !pip install https://github.com/Koziev/rutokenizer/archive/master.zip#egg=rutokenizer\n",
    "# !pip install pymystem3\n",
    "\n",
    "# !pip install nltk\n",
    "# nltk.download('punkt')\n",
    "# !wget https://raw.githubusercontent.com/mhq/train_punkt/master/russian.pickle -O ~/nltk_data/tokenizers/punkt/PY3/russian.pickle\n",
    "\n",
    "# Texterra\n",
    "# Можно ещё сравнивать с https://texterra.ispras.ru/products, но\n",
    "# 1. она медленно работает, как минимум затраты на http\n",
    "# 2. иногда кидает ошибку (возможно дело в английских предложениях)\n",
    "# 3. качество немного выше segtok\n",
    "\n",
    "# Polyglot\n",
    "# реализует http://www.unicode.org/reports/tr29/\n",
    "\n",
    "# Сорян, не смог установить. Дикие траблы с ICU\n",
    "# brew install icu4c\n",
    "# export ICU_VERSION=62.1\n",
    "# export BASE=/usr/local/Cellar/icu4c/\n",
    "# export PATH=$PATH:$BASE/$ICU_VERSION/bin\n",
    "# export PYICU_INCLUDES=$BASE/$ICU_VERSION/include\n",
    "# export PYICU_LFLAGS=-L$BASE/$ICU_VERSION/lib\n",
    "# pip install pyicu polyglot\n",
    "\n",
    "# Вроде установилось но \n",
    "# > from polyglot.text import Text\n",
    "# > Text('...')\n",
    "# Symbol not found: __ZNK6icu_6214Transliterator12getTargetSetERNS_10UnicodeSetE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for type in [SENT, TOKEN]:\n",
    "#     log(type)\n",
    "#     for model_name in MODELS[type]:\n",
    "#         log(model_name)\n",
    "#         model = MODELS[type][model_name]\n",
    "#         if is_class(model):\n",
    "#             model = model()\n",
    "#         model = Timing(model)\n",
    "\n",
    "#         for dataset_name in DATASETS:\n",
    "#             path = join_path(DATA_DIR, type, DATASET, dataset_name + JL + GZ)\n",
    "#             lines = load_gz_lines(path)\n",
    "#             records = parse_partitions(lines)\n",
    "#             records = head(records, 10000)\n",
    "#             records = log_progress(records, desc=dataset_name)\n",
    "\n",
    "#             records = (\n",
    "#                 Partition.from_substrings(model(_.text))\n",
    "#                 for _ in records\n",
    "#             )\n",
    "#             path = join_path(DATA_DIR, type, model_name, dataset_name + JL + GZ)\n",
    "#             lines = format_partitions(records)\n",
    "#             dump_gz_lines(lines, path)\n",
    "\n",
    "#             path = join_path(DATA_DIR, STATS + JL)\n",
    "#             record = [[type, model_name, dataset_name], model.time]\n",
    "#             lines = format_jl([record])\n",
    "#             append_lines(lines, path)\n",
    "\n",
    "#             model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff20785062640a5a40ffe7c7d318791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "keys = [\n",
    "    (type, model, dataset)\n",
    "    for type in [TOKEN, SENT]\n",
    "    for model in MODELS[type]\n",
    "    for dataset in DATASETS\n",
    "]\n",
    "for type, model, dataset in log_progress(keys):\n",
    "    path = join_path(DATA_DIR, type, DATASET, dataset + JL + GZ)\n",
    "    lines = load_gz_lines(path)\n",
    "    targets = parse_partitions(lines)\n",
    "\n",
    "    path = join_path(DATA_DIR, type, model, dataset + JL + GZ)\n",
    "    lines = load_gz_lines(path)\n",
    "    preds = parse_partitions(lines)\n",
    "\n",
    "    score = score_partitions(preds, targets)\n",
    "    scores[type, model, dataset] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = join_path(DATA_DIR, STATS + JL)\n",
    "lines = load_lines(path)\n",
    "items = parse_jl(lines)\n",
    "times = {\n",
    "    tuple(key): time\n",
    "    for key, time in items\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"0\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">corpora</th>\n",
       "      <th colspan=\"2\" halign=\"left\">syntag</th>\n",
       "      <th colspan=\"2\" halign=\"left\">gicrya</th>\n",
       "      <th colspan=\"2\" halign=\"left\">rnc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>errors</th>\n",
       "      <th>time</th>\n",
       "      <th>errors</th>\n",
       "      <th>time</th>\n",
       "      <th>errors</th>\n",
       "      <th>time</th>\n",
       "      <th>errors</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>re.findall(\\w+|\\d+|\\p+)</th>\n",
       "      <td>4217</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2914</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2402</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8630</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spacy</th>\n",
       "      <td>3283</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2639</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1742</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4010</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nltk.word_tokenize</th>\n",
       "      <td>5712</td>\n",
       "      <td>3.7</td>\n",
       "      <td>67523</td>\n",
       "      <td>3.9</td>\n",
       "      <td>12149</td>\n",
       "      <td>2.7</td>\n",
       "      <td>13564</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mystem</th>\n",
       "      <td>4280</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3624</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2515</td>\n",
       "      <td>3.6</td>\n",
       "      <td><b>1812</b></td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mosestokenizer</th>\n",
       "      <td><b>1188</b></td>\n",
       "      <td><b>2.0</b></td>\n",
       "      <td>1641</td>\n",
       "      <td><b>2.1</b></td>\n",
       "      <td>1696</td>\n",
       "      <td><b>1.7</b></td>\n",
       "      <td>2486</td>\n",
       "      <td><b>1.7</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segtok.word_tokenize</th>\n",
       "      <td>1491</td>\n",
       "      <td><b>2.4</b></td>\n",
       "      <td><b>1552</b></td>\n",
       "      <td><b>2.4</b></td>\n",
       "      <td><b>1657</b></td>\n",
       "      <td><b>1.8</b></td>\n",
       "      <td><b>1238</b></td>\n",
       "      <td><b>1.8</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aatimofeev/spacy_russian_tokenizer</th>\n",
       "      <td><b>1485</b></td>\n",
       "      <td>56.2</td>\n",
       "      <td><b>1225</b></td>\n",
       "      <td>53.3</td>\n",
       "      <td><b>630</b></td>\n",
       "      <td>39.2</td>\n",
       "      <td>2972</td>\n",
       "      <td>47.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koziev/rutokenizer</th>\n",
       "      <td>2744</td>\n",
       "      <td><b>1.1</b></td>\n",
       "      <td><b>1632</b></td>\n",
       "      <td><b>1.1</b></td>\n",
       "      <td>2576</td>\n",
       "      <td><b>0.9</b></td>\n",
       "      <td>9915</td>\n",
       "      <td><b>0.9</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>razdel.tokenize</th>\n",
       "      <td><b>1158</b></td>\n",
       "      <td>2.9</td>\n",
       "      <td>1861</td>\n",
       "      <td>3.0</td>\n",
       "      <td><b>315</b></td>\n",
       "      <td>2.0</td>\n",
       "      <td><b>2264</b></td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = report_table(scores, times, DATASETS, MODELS, TOKEN)\n",
    "html = format_report(table, github=True)\n",
    "patch_readme(TOKEN, html, README)\n",
    "patch_readme(TOKEN, html, RAZDEL_README)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"0\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">corpora</th>\n",
       "      <th colspan=\"2\" halign=\"left\">syntag</th>\n",
       "      <th colspan=\"2\" halign=\"left\">gicrya</th>\n",
       "      <th colspan=\"2\" halign=\"left\">rnc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>errors</th>\n",
       "      <th>time</th>\n",
       "      <th>errors</th>\n",
       "      <th>time</th>\n",
       "      <th>errors</th>\n",
       "      <th>time</th>\n",
       "      <th>errors</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>re.split([.?!…])</th>\n",
       "      <td>19974</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5986</td>\n",
       "      <td>0.4</td>\n",
       "      <td>9380</td>\n",
       "      <td>0.5</td>\n",
       "      <td>22483</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segtok.split_single</th>\n",
       "      <td>19450</td>\n",
       "      <td>16.5</td>\n",
       "      <td><b>4140</b></td>\n",
       "      <td>10.3</td>\n",
       "      <td>158672</td>\n",
       "      <td><b>1.5</b></td>\n",
       "      <td>172887</td>\n",
       "      <td><b>3.1</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mosestokenizer</th>\n",
       "      <td>60212</td>\n",
       "      <td>10.6</td>\n",
       "      <td>39361</td>\n",
       "      <td><b>5.4</b></td>\n",
       "      <td>12238</td>\n",
       "      <td>5.7</td>\n",
       "      <td>168743</td>\n",
       "      <td>385.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nltk.sent_tokenize</th>\n",
       "      <td><b>16346</b></td>\n",
       "      <td><b>8.8</b></td>\n",
       "      <td>4194</td>\n",
       "      <td><b>4.3</b></td>\n",
       "      <td><b>6774</b></td>\n",
       "      <td><b>4.2</b></td>\n",
       "      <td><b>32391</b></td>\n",
       "      <td><b>5.4</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deeppavlov/rusenttokenize</th>\n",
       "      <td><b>10138</b></td>\n",
       "      <td><b>9.9</b></td>\n",
       "      <td><b>1180</b></td>\n",
       "      <td>6.0</td>\n",
       "      <td><b>8402</b></td>\n",
       "      <td>5.6</td>\n",
       "      <td><b>20717</b></td>\n",
       "      <td>93.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>razdel.sentenize</th>\n",
       "      <td><b>9408</b></td>\n",
       "      <td><b>5.4</b></td>\n",
       "      <td><b>798</b></td>\n",
       "      <td><b>3.4</b></td>\n",
       "      <td><b>11020</b></td>\n",
       "      <td><b>3.6</b></td>\n",
       "      <td><b>10791</b></td>\n",
       "      <td><b>5.4</b></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = report_table(scores, times, DATASETS, MODELS, SENT)\n",
    "html = format_report(table, github=True)\n",
    "patch_readme(SENT, html, README)\n",
    "patch_readme(SENT, html, RAZDEL_README)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
